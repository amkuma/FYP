# -*- coding: utf-8 -*-
"""FYP2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dwGsuAwt9TRVP-YGaTSssNd7j7kul6CB
"""

from google.colab import drive
import os
import pandas as pd

drive.mount('/content/drive')

# Path to your preprocessed dataset in Google Drive
merged_file_path = '/content/drive/MyDrive/FYP/Dataset/final_dataset.csv'

# Load the preprocessed dataset
df = pd.read_csv(merged_file_path)

# Assuming 'text' is the column containing tweets, and 'label' is the target variable
X = df['cleaned_text']
y = df['label']

import tensorflow as tf

if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print("Please install GPU version of TF")

!pip install transformers

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from transformers import get_linear_schedule_with_warmup
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


def preprocess_text(text):
    if not isinstance(text, str):  # Checks if the text is not a string
        return ''  # Returns an empty string for non-string inputs
    text = text.lower()
    return text

#df['cleaned_text'] = df['text'].apply(preprocess_text)

# Remove rows where 'cleaned_text' is empty after cleaning
df = df[df['cleaned_text'].str.strip() != '']

# Split the dataset into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(df['cleaned_text'], df['label'], test_size=0.2)

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_texts = train_texts.fillna('').map(str)
val_texts = val_texts.fillna('').map(str)

# Tokenization
train_encodings = tokenizer(train_texts.to_list(), truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts.to_list(), truncation=True, padding=True, max_length=128)

# Custom dataset class
class BipolarDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = BipolarDataset(train_encodings, train_labels.to_list())
val_dataset = BipolarDataset(val_encodings, val_labels.to_list())

# Initialize BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Prepare for training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
model.to(device)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

optim = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_loader) * 4  # Number of epochs
scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=total_steps)

# Training loop
for epoch in range(4):  # Assuming you want to train for 4 epochs
    model.train()
    train_predictions, train_true_labels = [], []
    for batch in train_loader:
        optim.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optim.step()
        scheduler.step()

        logits = outputs.logits.detach().cpu().numpy()
        label_ids = labels.to('cpu').numpy()
        train_predictions.append(np.argmax(logits, axis=1).flatten())
        train_true_labels.append(label_ids.flatten())

    train_acc = np.mean(np.concatenate(train_predictions) == np.concatenate(train_true_labels))
    print(f'Epoch {epoch + 1}, Training Loss: {loss.item()}, Training Accuracy: {train_acc}')

    # Validation
    model.eval()
    val_predictions, val_true_labels = [], []
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1).cpu().numpy()
        labels = labels.cpu().numpy()
        val_predictions.extend(predictions)
        val_true_labels.extend(labels)

    val_acc = accuracy_score(val_true_labels, val_predictions)
    print(f'Epoch {epoch + 1}, Validation Accuracy: {val_acc}')

# Step 1: Setup Environment
# !pip install transformers
# !pip install nltk
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

# Ensure GPU is being used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')
if device.type == 'cuda':
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

def preprocess_text(text):
    # Check if the text is a string
    if not isinstance(text, str):
        return ''  # Returns an empty string for non-string inputs
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'@\w+|#\w+', '', text)
    emoticons = re.findall('[:;=]-?[)DPOp]', text)
    text = re.sub(r'[^a-zA-Z\s!]', '', text)
    text += ' ' + ' '.join(emoticons)
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

df['cleaned_text'] = df['cleaned_text'].apply(preprocess_text)
df = df[df['cleaned_text'].str.strip() != '']

# Tokenizer
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')

# Tokenization and attention mask generation
input_ids = []
attention_masks = []

for sent in df['cleaned_text']:
    encoded_dict = tokenizer.encode_plus(
        sent, add_special_tokens=True, max_length=64, pad_to_max_length=True,
        return_attention_mask=True, return_tensors='pt',
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(df.label.values)

# Split data into train and validation sets
batch_size = 32
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, attention_masks, random_state=2018, test_size=0.1)

# Create DataLoader for training and validation sets
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

# Setup ALBERT model
model = AlbertForSequenceClassification.from_pretrained("albert-base-v2", num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
epochs = 4
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Helper function for calculating accuracy
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

# Training and validation loop
for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')
    model.train()
    total_train_loss = 0
    total_train_accuracy = 0

    for step, batch in enumerate(train_dataloader):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        model.zero_grad()

        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs.loss
        logits = outputs.logits

        total_train_loss += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_train_accuracy += flat_accuracy(logits, label_ids)

    avg_train_loss = total_train_loss / len(train_dataloader)
    avg_train_accuracy = total_train_accuracy / len(train_dataloader)
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Accuracy: {0:.2f}".format(avg_train_accuracy))

    print("\nValidation...")
    model.eval()
    total_eval_accuracy = 0

    for batch in validation_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():
            outputs = model(b_input_ids, attention_mask=b_input_mask)
            logits = outputs.logits

        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, label_ids)

    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import re
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
import seaborn as sns  # For visualization of the confusion matrix
import numpy as np

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/FYP/Dataset/final_dataset.csv')

# Normalize text function
def normalize(data):
    normalized = []
    for i in data:
        i = str(i)  # Convert to string
        i = i.lower()  # Convert to lowercase
        i = re.sub('\\W', ' ', i)  # Remove non-word characters
        i = re.sub('\n', '', i)  # Remove newline characters
        i = re.sub(' +', ' ', i)  # Replace multiple spaces with a single space
        normalized.append(i)
    return normalized

# Preprocessing
features = normalize(df['cleaned_text'])
targets = df['label']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)

# Tokenization and Padding
max_vocab = 10000
maxlen = 256
tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Model Definition
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(max_vocab, 32, input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Early Stopping
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=1,
    restore_best_weights=True
)

# Training
batch_size = 64
epochs = 3
history = model.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.2,
    verbose=2,
    callbacks=[early_stopping]
)

# Evaluation
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"Test Loss: {test_loss}\nTest Accuracy: {test_acc}")

# Plot Training and Validation Accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Confusion Matrix Calculation
y_pred_prob = model.predict(X_test)
y_pred = np.round(y_pred_prob).astype(int)

# Plot Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Bipolar', 'Bipolar'], yticklabels=['Non-Bipolar', 'Bipolar'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Metrics Calculation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average=None)  # Returns precision for each class
recall = recall_score(y_test, y_pred, average=None)  # Returns recall for each class
f1 = f1_score(y_test, y_pred, average=None)  # Returns F1 score for each class

# Display Metrics
metrics_df = pd.DataFrame({
    'Metric': ['Precision', 'Recall', 'F1-Score'],
    'Non-Bipolar': [precision[0], recall[0], f1[0]],
    'Bipolar': [precision[1], recall[1], f1[1]]
})

print("\nMetrics for Each Class:")
print(metrics_df)

print(f"\nOverall Accuracy: {accuracy}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot the ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

model.save('/content/drive/MyDrive/FYP/Models/RNNModel.h5')

import json

# Serialize the tokenizer to a JSON string
tokenizer_json = tokenizer.to_json()

# Define the path where you want to save the tokenizer in your Google Drive
tokenizer_path = '/content/drive/MyDrive/FYP/Models/tokenizer.json'

# Write the JSON to a file in the specified path
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer_json)